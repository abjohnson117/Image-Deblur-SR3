{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/certifi/cacert.pem'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 946.4593505859375\n",
      "199 653.5341186523438\n",
      "299 452.61578369140625\n",
      "399 314.6488037109375\n",
      "499 219.80186462402344\n",
      "599 154.52517700195312\n",
      "699 109.55022430419922\n",
      "799 78.52891540527344\n",
      "899 57.1092529296875\n",
      "999 42.30370330810547\n",
      "1099 32.05937957763672\n",
      "1199 24.963970184326172\n",
      "1299 20.044734954833984\n",
      "1399 16.630950927734375\n",
      "1499 14.259708404541016\n",
      "1599 12.611133575439453\n",
      "1699 11.463973999023438\n",
      "1799 10.66506290435791\n",
      "1899 10.108219146728516\n",
      "1999 9.719802856445312\n",
      "Result: y = -0.028911108151078224 + 0.8445759415626526 x + 0.004987648688256741 x^2 + -0.09160003811120987 x^3\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "# Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to initialize backend 'METAL': INVALID_ARGUMENT: Unexpected PJRT_Api size: expected 592, got 560. Check installed software versions. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:593\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m   backend \u001b[39m=\u001b[39m _init_backend(platform)\n\u001b[1;32m    594\u001b[0m   _backends[platform] \u001b[39m=\u001b[39m backend\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:647\u001b[0m, in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    646\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mInitializing backend \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, platform)\n\u001b[0;32m--> 647\u001b[0m backend \u001b[39m=\u001b[39m registration\u001b[39m.\u001b[39;49mfactory()\n\u001b[1;32m    648\u001b[0m \u001b[39m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39m# factories instead of returning None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:463\u001b[0m, in \u001b[0;36mregister_plugin.<locals>.factory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThe library path is None when trying to dynamically load the\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m plugin.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n\u001b[0;32m--> 463\u001b[0m   xla_client\u001b[39m.\u001b[39;49mload_pjrt_plugin_dynamically(plugin_name, library_path)\n\u001b[1;32m    464\u001b[0m \u001b[39mreturn\u001b[39;00m xla_client\u001b[39m.\u001b[39mmake_c_api_client(plugin_name, options)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jaxlib/xla_client.py:130\u001b[0m, in \u001b[0;36mload_pjrt_plugin_dynamically\u001b[0;34m(plugin_name, library_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pjrt_plugin_dynamically\u001b[39m(plugin_name: \u001b[39mstr\u001b[39m, library_path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m   _xla\u001b[39m.\u001b[39;49mload_pjrt_plugin(plugin_name, library_path)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INVALID_ARGUMENT: Unexpected PJRT_Api size: expected 592, got 560. Check installed software versions.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# generate data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m jit_cube \u001b[38;5;241m=\u001b[39m jit(cube)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2161\u001b[0m, in \u001b[0;36mones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m   2159\u001b[0m shape \u001b[39m=\u001b[39m canonicalize_shape(shape)\n\u001b[1;32m   2160\u001b[0m dtypes\u001b[39m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[39m\"\u001b[39m\u001b[39mones\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2161\u001b[0m \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39;49mfull(shape, \u001b[39m1\u001b[39;49m, _jnp_dtype(dtype))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/lax/lax.py:1205\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype)\u001b[0m\n\u001b[1;32m   1203\u001b[0m weak_type \u001b[39m=\u001b[39m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dtypes\u001b[39m.\u001b[39mis_weakly_typed(fill_value)\n\u001b[1;32m   1204\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(dtype \u001b[39mor\u001b[39;00m _dtype(fill_value))\n\u001b[0;32m-> 1205\u001b[0m fill_value \u001b[39m=\u001b[39m _convert_element_type(fill_value, dtype, weak_type)\n\u001b[1;32m   1206\u001b[0m \u001b[39mreturn\u001b[39;00m broadcast(fill_value, shape)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/lax/lax.py:549\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[39mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    548\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_element_type_p\u001b[39m.\u001b[39;49mbind(operand, new_dtype\u001b[39m=\u001b[39;49mnew_dtype,\n\u001b[1;32m    550\u001b[0m                                      weak_type\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(weak_type))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/dispatch.py:132\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m   in_avals, in_shardings \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39munzip2([arg_spec(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args])\n\u001b[0;32m--> 132\u001b[0m   compiled_fun \u001b[39m=\u001b[39m xla_primitive_callable(\n\u001b[1;32m    133\u001b[0m       prim, in_avals, OrigShardings(in_shardings), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    134\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    135\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/util.py:284\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    283\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/util.py:277\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 277\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/dispatch.py:223\u001b[0m, in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, in_avals, orig_in_shardings, **params)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[39mreturn\u001b[39;00m out,\n\u001b[1;32m    222\u001b[0m donated_invars \u001b[39m=\u001b[39m (\u001b[39mFalse\u001b[39;00m,) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(in_avals)\n\u001b[0;32m--> 223\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(\n\u001b[1;32m    224\u001b[0m     lu\u001b[39m.\u001b[39;49mwrap_init(prim_fun), prim\u001b[39m.\u001b[39;49mname, donated_invars, \u001b[39mFalse\u001b[39;49;00m, in_avals,\n\u001b[1;32m    225\u001b[0m     orig_in_shardings)\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[1;32m    227\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/dispatch.py:250\u001b[0m, in \u001b[0;36m_xla_callable_uncached\u001b[0;34m(fun, name, donated_invars, keep_unused, in_avals, orig_in_shardings)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_xla_callable_uncached\u001b[39m(fun: lu\u001b[39m.\u001b[39mWrappedFun, name, donated_invars,\n\u001b[1;32m    249\u001b[0m                            keep_unused, in_avals, orig_in_shardings):\n\u001b[0;32m--> 250\u001b[0m   computation \u001b[39m=\u001b[39m sharded_lowering(\n\u001b[1;32m    251\u001b[0m       fun, name, donated_invars, keep_unused, \u001b[39mTrue\u001b[39;49;00m, in_avals, orig_in_shardings,\n\u001b[1;32m    252\u001b[0m       lowering_platform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    253\u001b[0m   \u001b[39mreturn\u001b[39;00m computation\u001b[39m.\u001b[39mcompile()\u001b[39m.\u001b[39munsafe_call\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/dispatch.py:242\u001b[0m, in \u001b[0;36msharded_lowering\u001b[0;34m(fun, name, donated_invars, keep_unused, inline, in_avals, in_shardings, lowering_platform)\u001b[0m\n\u001b[1;32m    237\u001b[0m in_shardings \u001b[39m=\u001b[39m [UNSPECIFIED \u001b[39mif\u001b[39;00m i \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m in_shardings]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# Pass in a singleton `UNSPECIFIED` for out_shardings because we don't know\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# the number of output avals at this stage. lower_sharding_computation will\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m# apply it to all out_avals.\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m \u001b[39mreturn\u001b[39;00m pxla\u001b[39m.\u001b[39;49mlower_sharding_computation(\n\u001b[1;32m    243\u001b[0m     fun, \u001b[39m'\u001b[39;49m\u001b[39mjit\u001b[39;49m\u001b[39m'\u001b[39;49m, name, in_shardings, UNSPECIFIED, donated_invars,\n\u001b[1;32m    244\u001b[0m     \u001b[39mtuple\u001b[39;49m(in_avals), keep_unused\u001b[39m=\u001b[39;49mkeep_unused, inline\u001b[39m=\u001b[39;49minline, always_lower\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    245\u001b[0m     devices_from_context\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, lowering_platform\u001b[39m=\u001b[39;49mlowering_platform)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:2031\u001b[0m, in \u001b[0;36mlower_sharding_computation\u001b[0;34m(fun_or_jaxpr, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, keep_unused, inline, always_lower, devices_from_context, lowering_platform)\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[39m# Device assignment across all inputs, outputs and shardings inside jaxpr\u001b[39;00m\n\u001b[1;32m   2029\u001b[0m \u001b[39m# should be the same.\u001b[39;00m\n\u001b[1;32m   2030\u001b[0m jaxpr_sharding \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dispatch\u001b[39m.\u001b[39mjaxpr_shardings(jaxpr))\n\u001b[0;32m-> 2031\u001b[0m backend, device_assignment \u001b[39m=\u001b[39m _get_and_check_device_assignment(\n\u001b[1;32m   2032\u001b[0m     it\u001b[39m.\u001b[39;49mchain([(i, MismatchType\u001b[39m.\u001b[39;49mARG_SHARDING, \u001b[39mNone\u001b[39;49;00m) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m in_shardings],\n\u001b[1;32m   2033\u001b[0m              [(o, MismatchType\u001b[39m.\u001b[39;49mOUT_SHARDING, \u001b[39mNone\u001b[39;49;00m) \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m out_shardings],\n\u001b[1;32m   2034\u001b[0m              [(js, MismatchType\u001b[39m.\u001b[39;49mSHARDING_INSIDE_COMPUTATION, source_info)\n\u001b[1;32m   2035\u001b[0m               \u001b[39mfor\u001b[39;49;00m js, source_info \u001b[39min\u001b[39;49;00m jaxpr_sharding]),\n\u001b[1;32m   2036\u001b[0m     devices_from_context)\n\u001b[1;32m   2038\u001b[0m committed \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(\n\u001b[1;32m   2039\u001b[0m     devices_from_context \u001b[39mor\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m     \u001b[39mlen\u001b[39m(device_assignment) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     \u001b[39many\u001b[39m(\u001b[39mnot\u001b[39;00m is_unspecified(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m in_shardings) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m     \u001b[39many\u001b[39m(\u001b[39mnot\u001b[39;00m is_unspecified(js) \u001b[39mfor\u001b[39;00m js, _ \u001b[39min\u001b[39;00m jaxpr_sharding) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m     \u001b[39many\u001b[39m(\u001b[39mnot\u001b[39;00m is_unspecified(o) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m out_shardings))\n\u001b[1;32m   2045\u001b[0m gs \u001b[39m=\u001b[39m sharding_impls\u001b[39m.\u001b[39mGSPMDSharding\u001b[39m.\u001b[39mget_replicated(device_assignment)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:1800\u001b[0m, in \u001b[0;36m_get_and_check_device_assignment\u001b[0;34m(shardings, devices)\u001b[0m\n\u001b[1;32m   1798\u001b[0m   final_device_assignment \u001b[39m=\u001b[39m devices\n\u001b[1;32m   1799\u001b[0m \u001b[39melif\u001b[39;00m first_sharding_info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1800\u001b[0m   final_device_assignment \u001b[39m=\u001b[39m (_get_default_device(),)\n\u001b[1;32m   1801\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m   final_device_assignment \u001b[39m=\u001b[39m first_sharding_info[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:1765\u001b[0m, in \u001b[0;36m_get_default_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_default_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m xc\u001b[39m.\u001b[39mDevice:\n\u001b[0;32m-> 1765\u001b[0m   \u001b[39mreturn\u001b[39;00m config\u001b[39m.\u001b[39mjax_default_device \u001b[39mor\u001b[39;00m xb\u001b[39m.\u001b[39;49mlocal_devices()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:790\u001b[0m, in \u001b[0;36mlocal_devices\u001b[0;34m(process_index, backend, host_id)\u001b[0m\n\u001b[1;32m    788\u001b[0m   process_index \u001b[39m=\u001b[39m host_id\n\u001b[1;32m    789\u001b[0m \u001b[39mif\u001b[39;00m process_index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 790\u001b[0m   process_index \u001b[39m=\u001b[39m get_backend(backend)\u001b[39m.\u001b[39mprocess_index()\n\u001b[1;32m    791\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m process_index \u001b[39m<\u001b[39m process_count()):\n\u001b[1;32m    792\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown process_index \u001b[39m\u001b[39m{\u001b[39;00mprocess_index\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:692\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39m@lru_cache\u001b[39m(maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# don't use util.memoize because there is no X64 dependence.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_backend\u001b[39m(\n\u001b[1;32m    690\u001b[0m     platform: Union[\u001b[39mNone\u001b[39;00m, \u001b[39mstr\u001b[39m, xla_client\u001b[39m.\u001b[39mClient] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    691\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m xla_client\u001b[39m.\u001b[39mClient:\n\u001b[0;32m--> 692\u001b[0m   \u001b[39mreturn\u001b[39;00m _get_backend_uncached(platform)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:673\u001b[0m, in \u001b[0;36m_get_backend_uncached\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    668\u001b[0m   \u001b[39mreturn\u001b[39;00m platform\n\u001b[1;32m    670\u001b[0m platform \u001b[39m=\u001b[39m (platform \u001b[39mor\u001b[39;00m FLAGS\u001b[39m.\u001b[39mjax_xla_backend \u001b[39mor\u001b[39;00m FLAGS\u001b[39m.\u001b[39mjax_platform_name\n\u001b[1;32m    671\u001b[0m             \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 673\u001b[0m bs \u001b[39m=\u001b[39m backends()\n\u001b[1;32m    674\u001b[0m \u001b[39mif\u001b[39;00m platform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m   platform \u001b[39m=\u001b[39m canonicalize_platform(platform)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.8/site-packages/jax/_src/xla_bridge.py:609\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    607\u001b[0m       \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m         err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 609\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    611\u001b[0m \u001b[39massert\u001b[39;00m _default_backend \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39m# We don't warn about falling back to CPU on Mac OS, because we don't\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m# support anything else there at the moment and warning would be pointless.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'METAL': INVALID_ARGUMENT: Unexpected PJRT_Api size: expected 592, got 560. Check installed software versions. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\t\t\t\t\t\t\t\t\n",
    "# define the cube function\n",
    "def cube(x):\n",
    "\treturn x * x * x\n",
    "\n",
    "# generate data\n",
    "x = jnp.ones((10000, 10000))\n",
    "\n",
    "jit_cube = jit(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.17 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
